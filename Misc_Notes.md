##  Note: All materials are from Amazon Web Services workshops.

## Deploying an LLM to Amazon SageMaker AI real-time endpoint
To use SageMaker AI endpoints, first deploy a managed endpoint through SageMaker Jumpstart, a feature that helps 
machine learning practitioners quickly get started with hundreds of production-ready models in SageMaker AI.
   
`(i)see examples on 0-setup as how to deploy ENDPOINT.`

`1-inference`
  - how to invoke an Amazon Bedrock model using AWS SDK for Python (`boto3`)
  - how to invoke an Amazon Bedrock model using LiteLLM
  - how to invoke a model hosted on Amazon SageMaker AI inference endpoints using AWS SDK for Python (`boto3`)
  - how to invoke a model hosted on Amazon SageMaker AI inference endpoints using the Amazon SageMaker Python SDK
  - how to invoke a model hosted on Amazon SageMaker AI inference endpoints using LiteLLM

## Tool Calling with Amazon Bedrock and Amazon SageMaker AI
The agent orchestrates these tools in sequence, creating workflows that seamlessly transition between different capabilities as needed. 
This approach allows systems to transcend the limitations of any single model or tool.

`2-tool-calling`
- how to perform tool calling with an Amazon Bedrock model using AWS SDK for Python (`boto3`)
- how to perform tool calling with an Amazon Bedrock model using LiteLLM
- how to perform tool calling with a model hosted on Amazon SageMaker AI inference endpoints using AWS SDK for Python (`boto3`)
- how to invoke a model hosted on Amazon SageMaker AI inference endpoints using the Amazon SageMaker Python SDK

  

